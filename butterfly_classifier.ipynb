{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19c3209",
   "metadata": {},
   "source": [
    "# (0) Download Butterfly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fffd35fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jovyan/.kaggle/kaggle.json'\n",
      "Downloading butterfly-images40-species.zip to /home/jovyan/work/project\n",
      " 99%|███████████████████████████████████████▌| 395M/399M [00:11<00:00, 44.1MB/s]\n",
      "100%|████████████████████████████████████████| 399M/399M [00:11<00:00, 36.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d gpiosenka/butterfly-images40-species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2410b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('butterfly-images40-species.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/butterfly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752480fc",
   "metadata": {},
   "source": [
    "# (1) Process Butterfly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eea0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43aa1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, path_label, transform=None):\n",
    "        self.path_label = path_label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.path_label[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "# Create dataset from raw structure obtained from Kaggle\n",
    "# (1): Download dataset fron Kaggle: https://www.kaggle.com/datasets/gpiosenka/butterfly-images40-species?select=train\n",
    "# (2): Unzip the downloaded dataset to './data/butterfly'\n",
    "# (3): Run the function\n",
    "def create_dataset(path = './data/butterfly/'):\n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.Resize((224,224)),\n",
    "                                    transforms.ToTensor()])\n",
    "    \n",
    "    train_path = path + 'train'\n",
    "    test_path = path + 'test'\n",
    "    \n",
    "    class_names=sorted(os.listdir(train_path))\n",
    "    N=list(range(len(class_names)))\n",
    "    normal_mapping=dict(zip(class_names,N)) \n",
    "    reverse_mapping=dict(zip(N,class_names))\n",
    "\n",
    "    paths0=[]\n",
    "    for dirname, _, filenames in os.walk(train_path):\n",
    "        for filename in filenames:\n",
    "            if filename[-4:]=='.jpg':\n",
    "                path=os.path.join(dirname, filename)\n",
    "                label=dirname.split('\\\\')[-1]\n",
    "                if label == '.ipynb_checkpoints':\n",
    "                    continue\n",
    "                paths0+=[(path,normal_mapping[label])]\n",
    "            \n",
    "    tpaths0=[]\n",
    "    for dirname, _, filenames in os.walk(test_path):\n",
    "        for filename in filenames:\n",
    "            if filename[-4:]=='.jpg':\n",
    "                path=os.path.join(dirname, filename)\n",
    "                label=dirname.split('\\\\')[-1]\n",
    "                if label == '.ipynb_checkpoints':\n",
    "                    continue\n",
    "                tpaths0+=[(path,normal_mapping[label])]\n",
    "\n",
    "    random.shuffle(paths0)            \n",
    "    random.shuffle(tpaths0)  \n",
    "\n",
    "    trainset = ImageDataset(paths0, transform)\n",
    "    testset = ImageDataset(tpaths0, transform)\n",
    "    \n",
    "    return trainset, testset, normal_mapping, reverse_mapping\n",
    "\n",
    "\n",
    "trainset, testset, normal_mapping, reverse_mapping = create_dataset()\n",
    "assert len(trainset) == 12594, 'Size of train set not match'\n",
    "assert len(testset) == 500, 'Size of test set not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277784b",
   "metadata": {},
   "source": [
    "# (2) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b3c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e1b3b6-25da-401c-b773-c0e70fb0194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, model='densenet121', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model, pretrained, in_chans=3)\n",
    "        self.fc1 = nn.Linear(1000,32)\n",
    "        self.fc2 = nn.Linear(32,64)        \n",
    "        self.fc3 = nn.Linear(64,num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d947ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, optimizer, criterion, epoch, device):\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (inp, label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inp, label = inp.to(device), label.to(device)\n",
    "        output = model(inp)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inp, label) in enumerate(test_loader):\n",
    "            inp, label = inp.to(device), label.to(device)\n",
    "            output = model(inp)\n",
    "            loss = criterion(output, label)\n",
    "            test_loss += loss.item()\n",
    "    return train_loss / len(train_loader), test_loss / len(test_loader)\n",
    "    \n",
    "\n",
    "# Params\n",
    "batch_size=100\n",
    "lr = 0.0001\n",
    "device='cuda'\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=batch_size)\n",
    "\n",
    "# Model\n",
    "model = DenseNet121(num_classes=len(normal_mapping))\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# load states\n",
    "# model.load_state_dict(torch.load('butterfly_classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b6966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 1/5 [01:11<04:44, 71.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train: 3.1463764139584134, Test: 1.0290743589401246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▌                                                  | 2/5 [01:55<02:46, 55.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train: 0.6463653444061204, Test: 0.2617968559265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [02:38<01:39, 49.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train: 0.18041037442901778, Test: 0.20863890647888184\n"
     ]
    }
   ],
   "source": [
    "train_hist = list()\n",
    "test_hist = list()\n",
    "for epoch in trange(1, 5 + 1):\n",
    "    train_loss, test_loss = train(model, train_loader, test_loader, optimizer, criterion, epoch, device=device)\n",
    "    train_hist.append(train_loss)\n",
    "    test_hist.append(test_loss)\n",
    "    print('Epoch {}: Train: {}, Test: {}'.format(epoch, train_loss, test_loss))\n",
    "    \n",
    "torch.save(model.state_dict(), 'butterfly_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270df7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
