{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53614e47",
   "metadata": {},
   "source": [
    "# (0) Download ImageNet (Tiny) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f71179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jovyan/.kaggle/kaggle.json'\n",
      "Downloading butterfly-images40-species.zip to /home/jovyan/work/project\n",
      " 99%|███████████████████████████████████████▌| 395M/399M [00:11<00:00, 44.1MB/s]\n",
      "100%|████████████████████████████████████████| 399M/399M [00:11<00:00, 36.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d akash2sharma/tiny-imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01317298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('tiny-imagenet.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/tiny_imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83799c64",
   "metadata": {},
   "source": [
    "# (1) Process TinyImageNet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca61ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "import os\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, path_label, transform=None):\n",
    "        self.path_label = path_label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.path_label[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "# Create dataset from raw structure obtained from Kaggle\n",
    "# (1): Download dataset fron Kaggle: https://www.kaggle.com/datasets/gpiosenka/butterfly-images40-species?select=train\n",
    "# (2): Unzip the downloaded dataset to './data/butterfly'\n",
    "# (3): Run the function\n",
    "def create_butterfly_dataset(path = './data/butterfly/', img_reshape=(3, 224, 224)):\n",
    "    C, H, W = img_reshape\n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.Resize((H, W)),\n",
    "                                    transforms.ToTensor()])\n",
    "      \n",
    "    train_path = path + 'train'\n",
    "    test_path = path + 'test'\n",
    "    \n",
    "    class_names=sorted(os.listdir(train_path))\n",
    "    N=list(range(len(class_names)))\n",
    "    normal_mapping=dict(zip(class_names,N)) \n",
    "    reverse_mapping=dict(zip(N,class_names))\n",
    "\n",
    "    paths0=[]\n",
    "    # store an image for each class for adversarial attack\n",
    "    sample_img_dataset = torch.zeros((len(class_names), C, H, W))\n",
    "    seen = list()\n",
    "    for dirname, _, filenames in os.walk(train_path):\n",
    "        for filename in filenames:\n",
    "            if filename[-4:]=='.jpg':\n",
    "                path=os.path.join(dirname, filename)\n",
    "                label=dirname.split('/')[-1]\n",
    "                if label == '.ipynb_checkpoints':\n",
    "                    continue\n",
    "                paths0+=[(path,normal_mapping[label])]\n",
    "            if label not in seen:\n",
    "                image = Image.open(path).convert('RGB')\n",
    "                image = transform(image)\n",
    "                sample_img_dataset[normal_mapping[label], :, :, :] = image\n",
    "                seen.append(label)\n",
    "            \n",
    "    tpaths0=[]\n",
    "    for dirname, _, filenames in os.walk(test_path):\n",
    "        for filename in filenames:\n",
    "            if filename[-4:]=='.jpg':\n",
    "                path=os.path.join(dirname, filename)\n",
    "                label=dirname.split('/')[-1]\n",
    "                if label == '.ipynb_checkpoints':\n",
    "                    continue\n",
    "                tpaths0+=[(path,normal_mapping[label])]\n",
    "                \n",
    "    random.seed(123)\n",
    "    random.shuffle(paths0)            \n",
    "    random.shuffle(tpaths0)  \n",
    "\n",
    "    trainset = ImageDataset(paths0, transform)\n",
    "    testset = ImageDataset(tpaths0, transform)\n",
    "    \n",
    "    return trainset, testset, normal_mapping, reverse_mapping, sample_img_dataset\n",
    "\n",
    "def create_imagenet_dataset(path='./data/tiny_imagenet/tiny-imagenet-200/', img_reshape=(3, 224, 224), split_ratio = 0.8, num_classes = 10):\n",
    "    C, H, W = img_reshape\n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.Resize((H, W)),\n",
    "                                    transforms.ToTensor()])\n",
    "      \n",
    "    data_path = path + 'train'\n",
    "    \n",
    "    class_names = sorted(os.listdir(data_path))\n",
    "    N = list(range(len(class_names)))\n",
    "    normal_mapping = dict(zip(class_names, N)) \n",
    "    reverse_mapping = dict(zip(N, class_names))\n",
    "\n",
    "    paths0 = []\n",
    "    sample_img_dataset = torch.zeros((len(class_names), C, H, W))\n",
    "    seen = list()\n",
    "\n",
    "    for label in class_names:\n",
    "        if len(seen) > num_classes:\n",
    "            break\n",
    "        label_path = os.path.join(data_path, label, 'images')\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        for filename in os.listdir(label_path):\n",
    "            if filename.endswith('.JPEG'):\n",
    "                path = os.path.join(label_path, filename)\n",
    "                paths0.append((path, normal_mapping[label]))\n",
    "                if label not in seen:\n",
    "                    image = Image.open(path).convert('RGB')\n",
    "                    image = transform(image)\n",
    "                    sample_img_dataset[normal_mapping[label], :, :, :] = image\n",
    "                    seen.append(label)\n",
    "                \n",
    "    random.seed(123)\n",
    "    random.shuffle(paths0) \n",
    "    \n",
    "    data = ImageDataset(paths0, transform)\n",
    "\n",
    "    total_size = len(data)\n",
    "    train_size = int(split_ratio * total_size)\n",
    "    test_size = total_size - train_size\n",
    "\n",
    "    trainset, testset = random_split(data, [train_size, test_size])\n",
    "\n",
    "    return trainset, testset, normal_mapping, reverse_mapping, sample_img_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c8c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import create_imagenet_dataset\n",
    "trainset, testset, normal_mapping, reverse_mapping, sample_img_dataset = create_imagenet_dataset()\n",
    "assert len(trainset) == 80000, 'Size of train set not match'\n",
    "assert len(testset) == 20000, 'Size of test set not match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f14c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c57936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05f8f9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot find v0.10. in https://github.com/pytorch/vision. If it's a commit from a forked repo, please call hub.load() with forked repo directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch/vision:v0.10.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minception_v3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:555\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown source: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Allowed values: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m \u001b[43m_get_cache_or_reload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_validation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:222\u001b[0m, in \u001b[0;36m_get_cache_or_reload\u001b[1;34m(github, force_reload, trust_repo, calling_fn, verbose, skip_validation)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# Validate the tag/branch is from the original repo instead of a forked repo\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_validation:\n\u001b[1;32m--> 222\u001b[0m         \u001b[43m_validate_not_a_forked_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_owner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     cached_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hub_dir, normalized_br \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    225\u001b[0m     _remove_if_exists(cached_file)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:189\u001b[0m, in \u001b[0;36m_validate_not_a_forked_repo\u001b[1;34m(repo_owner, repo_name, ref)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m br[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m ref \u001b[38;5;129;01mor\u001b[39;00m br[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommit\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msha\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(ref):\n\u001b[0;32m    187\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in https://github.com/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_owner\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    190\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf it\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms a commit from a forked repo, please call hub.load() with forked repo directly.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot find v0.10. in https://github.com/pytorch/vision. If it's a commit from a forked repo, please call hub.load() with forked repo directly."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.','inception_v3', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4645757",
   "metadata": {},
   "source": [
    "# (2) Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b85fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.butterfly_classifier import DenseNet121\n",
    "from utils.base import train_classifier\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Params\n",
    "batch_size=100\n",
    "lr = 0.0001\n",
    "device='cuda'\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=batch_size)\n",
    "\n",
    "# Model\n",
    "model = DenseNet121(num_classes=len(normal_mapping)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# load states\n",
    "# model.load_state_dict(torch.load('./model/states/butterfly_classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "train_hist = list()\n",
    "test_hist = list()\n",
    "for epoch in trange(1, 5 + 1):\n",
    "    train_loss, test_loss = train_classifier(model, train_loader, test_loader, optimizer, criterion, epoch)\n",
    "    train_hist.append(train_loss)\n",
    "    test_hist.append(test_loss)\n",
    "    print('Epoch {}: Train: {}, Test: {}'.format(epoch, train_loss, test_loss))\n",
    "    \n",
    "torch.save(model.state_dict(), 'butterfly_classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb99bc",
   "metadata": {},
   "source": [
    "# (3) Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.base import eval_accuracy\n",
    "\n",
    "\n",
    "acc = eval_accuracy(model, testset,  batch_size=100)\n",
    "print('Accuracy on test set is {}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
