{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2299f451",
   "metadata": {},
   "source": [
    "# (0) Download Butterfly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e43057",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d gpiosenka/butterfly-images40-species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd14509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('butterfly-images40-species.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/butterfly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0033965e",
   "metadata": {},
   "source": [
    "# (1) Process Butterfly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5e6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import create_butterfly_dataset\n",
    "trainset, testset, normal_mapping, reverse_mapping = create_butterfly_dataset()\n",
    "assert len(trainset) == 12594, 'Size of train set not match'\n",
    "assert len(testset) == 500, 'Size of test set not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a8bd5",
   "metadata": {},
   "source": [
    "# (2) Import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bd1ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.butterfly_classifier import DenseNet121\n",
    "import torch\n",
    "\n",
    "model = DenseNet121(num_classes=len(normal_mapping)).to('cuda')\n",
    "model.load_state_dict(torch.load('./model/states/butterfly_classifier.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40a5a8",
   "metadata": {},
   "source": [
    "# (3) Evaluate Untargeted Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42d0c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.9640000462532043\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.base import eval_accuracy\n",
    "\n",
    "\n",
    "acc = eval_accuracy(model, testset,  batch_size=100)\n",
    "print('Accuracy on test set is {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc147729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo.attacker import adversarial_generator\n",
    "\n",
    "test_img, test_label = testset[5]\n",
    "test_img = test_img.unsqueeze(0)\n",
    "\n",
    "adv_img = adversarial_generator(model = model, target_class=test_label,\n",
    "                             image=test_img, \n",
    "                             search_var=1e-3,\n",
    "                             sample_num=50,\n",
    "                             bound=0.1,\n",
    "                             lr=0.01,\n",
    "                             query_limit=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c53973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "adv_logits = model(adv_img)\n",
    "org_logits = model(test_img.to('cuda'))\n",
    "\n",
    "print('Adversarial: predicted class is {}'.format(torch.argmax(adv_logits, dim=1)))\n",
    "print('Original: predicted class is {}'.format(torch.argmax(org_logits, dim=1)))\n",
    "\n",
    "print('Adversarial: logit of true class is {}'.format(adv_logits[0, test_label]))\n",
    "print('Original: logit of true class is {}'.format(org_logits[0, test_label]))\n",
    "\n",
    "print('Adversarial: probability of true class is {}'.format(F.softmax(adv_logits, dim=1)[0, test_label]))\n",
    "print('Original: probability of true class is {}'.format(F.softmax(org_logits, dim=1)[0, test_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9666add8-a5e7-4773-b80a-b39cd9521df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "def NES(model, target_class, image, search_var, sample_num, g, u):\n",
    "    #parameters\n",
    "    n = sample_num #should be even\n",
    "    N = image.size(2) #assume the image is N x N may subject to change\n",
    "    \n",
    "    #NES estimation\n",
    "    g.zero_()\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            u.normal_()\n",
    "            g = g + F.softmax(model(image + search_var * u), dim =1)[0,target_class] * u\n",
    "            g = g - F.softmax(model(image - search_var * u), dim =1)[0,target_class] * u #we assume the output of the model is ordered by class index\n",
    "    return 1 / (2*n*search_var) * g\n",
    "\n",
    "def PIA_adversarial_generator(model, initial_image, image, target_class, epsilon_adv, epsilon_0, search_var, sample_num, delta_epsilon, eta_max, eta_min, k=5):\n",
    "    device = next(model.parameters()).device\n",
    "    initial_image, image = initial_image.to(device), image.to(device)\n",
    "    x_adv = image.clone()\n",
    "    N = initial_image.size(2)\n",
    "    g = torch.zeros(N, requires_grad=False).to(device)\n",
    "    u = torch.randn((N, N)).to(device)\n",
    "    \n",
    "    epsilon = epsilon_0\n",
    "    x_adv = torch.clamp(x_adv, initial_image - epsilon, initial_image + epsilon)\n",
    "    new_class = target_class\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        time = 1\n",
    "        while (epsilon > epsilon_adv) | (new_class != target_class):\n",
    "            if time%20 == 0:\n",
    "                print(\"20 times\")\n",
    "                print(F.softmax(model(x_adv), dim=1))\n",
    "                print(torch.argmax(model(x_adv)))\n",
    "                print(epsilon)\n",
    "            time += 1\n",
    "            gradient = NES(model, target_class, x_adv, search_var, sample_num, g, u)\n",
    "            eta = eta_max\n",
    "            x_adv_hat = x_adv - eta * gradient\n",
    "            probabilities_adv = F.softmax(model(x_adv_hat), dim=1)\n",
    "            top_probs, top_classes = torch.topk(probabilities_adv, k)\n",
    "            while not target_class in top_classes[0]:\n",
    "                if eta < eta_min:\n",
    "                    epsilon += delta_epsilon\n",
    "                    delta_epsilon /= 2\n",
    "                    x_adv_hat = x_adv\n",
    "                    break  \n",
    "                eta /= 2\n",
    "                x_adv_hat = torch.clamp(x_adv - eta * gradient, initial_image - epsilon, initial_image + epsilon)\n",
    "            x_adv = x_adv_hat\n",
    "            epsilon = epsilon - delta_epsilon\n",
    "            new_class = torch.argmax(model(x_adv))\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409bf16a-bbe4-4072-adf9-4bef6b41fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_label = testset[3]\n",
    "test_img = test_img.unsqueeze(0)\n",
    "initial_img, initial_label = testset[111]\n",
    "initial_img = initial_img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73033e9-5972-4ad4-b9df-61dea8cdf249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(initial_label)\n",
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9d39ac-04a8-4e9a-828f-13ea42c542ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "torch.argmax(model(initial_img.to('cuda')), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bac7abe-ee93-4592-97f6-c83037b247b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 times\n",
      "tensor([[2.0374e-06, 1.1924e-04, 4.2600e-07, 2.7792e-05, 1.4960e-05, 4.5288e-08,\n",
      "         1.0205e-01, 1.0875e-03, 1.2743e-06, 2.4363e-03, 6.1711e-07, 4.9238e-07,\n",
      "         5.8783e-08, 3.7343e-07, 2.1906e-05, 2.6281e-07, 7.9728e-05, 3.0662e-08,\n",
      "         5.8256e-06, 4.9032e-07, 2.5062e-04, 2.2484e-06, 6.2691e-05, 1.8416e-04,\n",
      "         6.3189e-05, 2.9964e-04, 9.2142e-07, 8.2076e-09, 1.1231e-06, 1.8921e-07,\n",
      "         2.5577e-07, 8.0916e-04, 1.0462e-05, 1.7859e-04, 1.6533e-04, 9.5686e-07,\n",
      "         5.4465e-06, 2.6835e-06, 2.4141e-04, 1.3511e-05, 8.3550e-05, 6.8761e-07,\n",
      "         3.6833e-04, 5.4205e-04, 5.7056e-06, 9.5582e-05, 9.4168e-07, 9.3150e-03,\n",
      "         1.0687e-05, 4.8317e-07, 3.3483e-04, 4.9779e-06, 8.4849e-07, 1.7952e-06,\n",
      "         1.0313e-05, 8.9764e-06, 5.3263e-07, 8.6556e-08, 2.6397e-02, 1.6718e-08,\n",
      "         1.4010e-05, 1.1547e-07, 6.9851e-07, 1.1322e-07, 3.9558e-05, 2.8570e-04,\n",
      "         1.2209e-07, 1.0742e-06, 2.8430e-07, 6.4690e-04, 2.4013e-03, 8.6092e-05,\n",
      "         4.6606e-07, 3.4864e-04, 7.0995e-06, 2.4246e-05, 9.2847e-07, 3.5235e-04,\n",
      "         6.2473e-06, 6.4811e-07, 4.1410e-03, 8.2462e-01, 1.1300e-04, 7.4745e-05,\n",
      "         5.9535e-05, 8.5223e-05, 6.3436e-08, 4.8683e-06, 3.8936e-07, 4.3536e-09,\n",
      "         2.2998e-05, 7.8811e-06, 7.0428e-07, 9.6493e-06, 2.0141e-02, 1.1138e-06,\n",
      "         1.1282e-03, 1.7888e-07, 4.8530e-05, 1.1790e-06]], device='cuda:0')\n",
      "tensor(81, device='cuda:0')\n",
      "0.481\n",
      "20 times\n",
      "tensor([[8.9833e-07, 2.6767e-04, 1.5843e-07, 1.3571e-05, 8.8734e-06, 2.0065e-08,\n",
      "         1.1355e-02, 8.5428e-04, 1.2328e-06, 7.6641e-04, 9.3526e-07, 1.6690e-07,\n",
      "         4.3073e-08, 3.3227e-07, 1.9114e-05, 1.9549e-07, 3.7057e-05, 2.3591e-08,\n",
      "         1.2671e-05, 1.2260e-07, 1.7930e-04, 4.9150e-07, 6.4080e-05, 1.9197e-04,\n",
      "         1.2314e-04, 1.9198e-04, 4.3094e-07, 9.5265e-09, 6.4461e-07, 2.9814e-07,\n",
      "         1.9803e-07, 9.2158e-04, 3.9032e-06, 8.0020e-05, 1.5787e-04, 9.0017e-07,\n",
      "         8.0209e-06, 1.8767e-06, 7.1533e-05, 1.2912e-05, 6.5879e-05, 1.8350e-06,\n",
      "         2.9665e-04, 2.1761e-04, 1.0360e-05, 4.7180e-05, 6.7752e-07, 4.9735e-03,\n",
      "         5.9173e-06, 1.6796e-07, 1.4596e-04, 4.3078e-06, 1.1191e-06, 1.3424e-06,\n",
      "         9.5780e-06, 1.1066e-05, 2.8323e-07, 7.1871e-08, 2.2896e-02, 2.1489e-08,\n",
      "         1.7115e-05, 8.0369e-08, 3.3297e-07, 1.8268e-07, 4.9923e-05, 1.0786e-04,\n",
      "         7.9733e-08, 1.1973e-06, 1.7052e-07, 2.8896e-04, 1.0654e-03, 4.4722e-05,\n",
      "         3.1399e-07, 1.4736e-04, 3.9684e-06, 1.6462e-05, 3.1082e-07, 1.0861e-04,\n",
      "         3.9899e-06, 4.3561e-07, 3.8354e-03, 9.3767e-01, 8.6996e-05, 5.0728e-05,\n",
      "         5.7191e-05, 2.1511e-05, 8.9121e-08, 4.5352e-06, 3.0986e-07, 8.7483e-09,\n",
      "         2.0124e-05, 6.9098e-06, 8.4547e-07, 4.6940e-06, 1.1111e-02, 5.7816e-07,\n",
      "         1.1946e-03, 1.1544e-07, 3.4363e-05, 1.2422e-06]], device='cuda:0')\n",
      "tensor(81, device='cuda:0')\n",
      "0.46099999999999997\n",
      "20 times\n",
      "tensor([[7.5258e-07, 3.0459e-04, 1.3026e-07, 1.2014e-05, 7.7516e-06, 1.6710e-08,\n",
      "         7.6879e-03, 8.2876e-04, 1.1798e-06, 6.2833e-04, 9.6773e-07, 1.3720e-07,\n",
      "         3.9785e-08, 3.1308e-07, 1.8646e-05, 1.8346e-07, 3.2059e-05, 2.1067e-08,\n",
      "         1.3487e-05, 9.4698e-08, 1.7437e-04, 3.7470e-07, 6.1620e-05, 1.8456e-04,\n",
      "         1.3414e-04, 1.6824e-04, 3.7071e-07, 9.4288e-09, 5.8889e-07, 3.1024e-07,\n",
      "         1.8161e-07, 9.2317e-04, 3.2457e-06, 6.8483e-05, 1.5475e-04, 9.0717e-07,\n",
      "         8.7098e-06, 1.7199e-06, 5.5999e-05, 1.2872e-05, 5.9850e-05, 1.9988e-06,\n",
      "         2.8645e-04, 1.8246e-04, 1.1158e-05, 4.3205e-05, 5.9283e-07, 4.4291e-03,\n",
      "         4.9914e-06, 1.3324e-07, 1.2935e-04, 3.9509e-06, 1.0908e-06, 1.1837e-06,\n",
      "         9.4425e-06, 1.1129e-05, 2.5187e-07, 6.9591e-08, 2.1812e-02, 2.1620e-08,\n",
      "         1.6454e-05, 7.0713e-08, 2.8302e-07, 1.9071e-07, 5.0742e-05, 9.3223e-05,\n",
      "         7.1774e-08, 1.1559e-06, 1.5306e-07, 2.4621e-04, 9.2366e-04, 4.1357e-05,\n",
      "         2.9124e-07, 1.2170e-04, 3.4719e-06, 1.4724e-05, 2.5977e-07, 8.1853e-05,\n",
      "         3.6130e-06, 4.0464e-07, 3.7426e-03, 9.4497e-01, 7.7193e-05, 4.6932e-05,\n",
      "         5.3799e-05, 1.6773e-05, 9.1885e-08, 4.1935e-06, 2.8920e-07, 9.2157e-09,\n",
      "         1.9377e-05, 6.7226e-06, 8.3832e-07, 3.9918e-06, 9.7584e-03, 5.0097e-07,\n",
      "         1.1810e-03, 1.0576e-07, 3.2252e-05, 1.2133e-06]], device='cuda:0')\n",
      "tensor(81, device='cuda:0')\n",
      "0.44099999999999995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m adv_img \u001b[38;5;241m=\u001b[39m \u001b[43mPIA_adversarial_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_image\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mepsilon_adv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msearch_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdelta_epsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43meta_max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_min\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 40\u001b[0m, in \u001b[0;36mPIA_adversarial_generator\u001b[1;34m(model, initial_image, image, target_class, epsilon_adv, epsilon_0, search_var, sample_num, delta_epsilon, eta_max, eta_min, k)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(epsilon)\n\u001b[0;32m     39\u001b[0m time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 40\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43mNES\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m eta \u001b[38;5;241m=\u001b[39m eta_max\n\u001b[0;32m     42\u001b[0m x_adv_hat \u001b[38;5;241m=\u001b[39m x_adv \u001b[38;5;241m-\u001b[39m eta \u001b[38;5;241m*\u001b[39m gradient\n",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mNES\u001b[1;34m(model, target_class, image, search_var, sample_num, g, u)\u001b[0m\n\u001b[0;32m     14\u001b[0m         u\u001b[38;5;241m.\u001b[39mnormal_()\n\u001b[0;32m     15\u001b[0m         g \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(model(image \u001b[38;5;241m+\u001b[39m search_var \u001b[38;5;241m*\u001b[39m u), dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m,target_class] \u001b[38;5;241m*\u001b[39m u\n\u001b[1;32m---> 16\u001b[0m         g \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msearch_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m, dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m,target_class] \u001b[38;5;241m*\u001b[39m u \u001b[38;5;66;03m#we assume the output of the model is ordered by class index\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mn\u001b[38;5;241m*\u001b[39msearch_var) \u001b[38;5;241m*\u001b[39m g\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\github\\BlackBoxAttack\\model\\butterfly_classifier.py:18\u001b[0m, in \u001b[0;36mDenseNet121.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\timm\\models\\densenet.py:302\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 302\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_pool(x)\n\u001b[0;32m    304\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_drop(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\timm\\models\\densenet.py:299\u001b[0m, in \u001b[0;36mDenseNet.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\timm\\models\\densenet.py:124\u001b[0m, in \u001b[0;36mDenseBlock.forward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    122\u001b[0m features \u001b[38;5;241m=\u001b[39m [init_features]\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 124\u001b[0m     new_features \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(new_features)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\timm\\models\\densenet.py:88\u001b[0m, in \u001b[0;36mDenseLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m new_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(bottleneck_output))\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\timm\\models\\densenet.py:46\u001b[0m, in \u001b[0;36mDenseLayer.bottleneck_fn\u001b[1;34m(self, xs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbottleneck_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# type: (List[torch.Tensor]) -> torch.Tensor\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     concated_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(xs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcated_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: T484\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bottleneck_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adv_img = PIA_adversarial_generator(model, initial_image = initial_img, \n",
    "                                    image = test_img, target_class = test_label,\n",
    "                                    epsilon_adv = 0.05, epsilon_0 = 0.5,\n",
    "                                    search_var = 1e-3, sample_num = 50,\n",
    "                                    delta_epsilon = 1e-3, \n",
    "                                    eta_max = 0.01, eta_min = 0.005,\n",
    "                                    k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5a376-f17c-4bab-ab7d-12ea5cae3bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
