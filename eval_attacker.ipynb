{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c12f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.base import eval_accuracy\n",
    "from utils.base import get_correct_predictions_subset\n",
    "from utils.data import create_butterfly_dataset\n",
    "from utils.data import create_imagenet_dataset\n",
    "from model.butterfly_classifier import DenseNet121\n",
    "from algo.attacker import adversarial_generator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2cc1266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n"
     ]
    }
   ],
   "source": [
    "if torch.tensor([23]) != 1:\n",
    "    print('aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.argmax(matches, dim=1)\n",
    "indices = indices + 1\n",
    "indices[~matches.any(dim=1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe44b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[~matches.any(dim=1)] = 0\n",
    "indices.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26016ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (idx == a).to(int)\n",
    "# indices: (batch_size, 1)\n",
    "indices = torch.argmax(matches, dim=1)\n",
    "indices = indices + 1\n",
    "indices[~matches.any(dim=1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc228f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "(idx == 0).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d763d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(idx[0] == a).nonzero(as_tuple = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = idx[:, 0][4] \n",
    "def get_rank(x, indices):\n",
    "    vals = x[range(len(x)), indices]\n",
    "    return (x > vals[:, None]).long().sum(1)\n",
    "get_rank(idx, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff7697",
   "metadata": {},
   "source": [
    "# (0) Download Butterfly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d gpiosenka/butterfly-images40-species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72378467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('butterfly-images40-species.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/butterfly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26846483",
   "metadata": {},
   "source": [
    "# (1) Process Butterfly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699e46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, normal_mapping, reverse_mapping, sample_img_dataset = create_butterfly_dataset()\n",
    "assert len(trainset) == 12594, 'Size of train set not match'\n",
    "assert len(testset) == 500, 'Size of test set not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa8ca7",
   "metadata": {},
   "source": [
    "# (2) Import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c3997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DenseNet121(num_classes=len(normal_mapping)).to('cuda')\n",
    "model.load_state_dict(torch.load('./model/states/butterfly_classifier.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad53eb",
   "metadata": {},
   "source": [
    "# (3) Evaluate Untargeted Adversarial Examples\n",
    "\n",
    "We also subset the parts where the model could provide correct predictions to attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b21759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.9640\n",
      "Number of correctly predicted samples: 482\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy, correct_subset = get_correct_predictions_subset(model, testset, batch_size=100)\n",
    "print('Accuracy on test set is {:.4f}'.format(accuracy))\n",
    "print('Number of correctly predicted samples:', len(correct_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6689fe9",
   "metadata": {},
   "source": [
    "# (4) Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4238850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters definition\n",
    "batch_size = 32\n",
    "query_limit = 30000 # max attack limit \n",
    "search_var = 1e-3 # amount to perturb the input image\n",
    "sample_num = 50 # 2*sample_num for estimating gradient\n",
    "bound = 0.1 # the l-infinity distance between the adversarial example and the input image\n",
    "# partial information paramters + lebel-only parameters\n",
    "epsilon = 0.5 # initial searching range from the target image\n",
    "delta = 0.01 # rate to decrease epsilon\n",
    "eta_max = 0.02 # maximum learning rate\n",
    "eta_min = 0.01 # minimum learning rate\n",
    "k = 5 # information access\n",
    "# label-only parameter\n",
    "mu = 0.001 # radius for sampling ball\n",
    "m = 50 # 2*number of sample for proxy score\n",
    "# test_loader = DataLoader(correct_subset, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(correct_subset, batch_size = 32, shuffle = False)\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d05da",
   "metadata": {},
   "source": [
    "# (5) Query-Limited Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count = 0\n",
    "query_counts = []\n",
    "adv_images = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[0]\n",
    "        target_classes = batch[1]\n",
    "        adv_image_batch, query_count_batch = adversarial_generator(model, target_classes, images, \n",
    "                                                                 search_var, sample_num,\n",
    "                                                                bound, lr, query_limit)\n",
    "        query_counts.append(query_count_batch)\n",
    "        adv_images.append(adv_image_batch)\n",
    "        # count the number of success attack\n",
    "        success_count += sum(query_count <= query_limit for query_count in query_count_batch)\n",
    "        print(success_count)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_image_all = torch.concat(adv_images, dim = 0)\n",
    "query_count_all = torch.concat(query_counts, dim = 0)\n",
    "torch.save(adv_image_all, \"adv_img.pt\")\n",
    "torch.save(query_count_all, \"query.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d888eec",
   "metadata": {},
   "source": [
    "# (6) Partial-Information Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3ab39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [42:46<00:00,  5.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from algo.attacker import PIA_adversarial_generator\n",
    "from algo.defender import PartialInfo\n",
    "from tqdm import trange\n",
    "\n",
    "success_count_PIA = 0\n",
    "query_counts_PIA = []\n",
    "adv_images_PIA = []\n",
    "\n",
    "top_k_output_model = PartialInfo(model=model, k=k)\n",
    "with torch.no_grad():\n",
    "    for i in trange(len(testset)):\n",
    "        \n",
    "        images = testset[i][0].unsqueeze(0)\n",
    "        # images = batch[0]\n",
    "        adv_image_batch, query_count_batch = PIA_adversarial_generator(top_k_output_model, images, sample_img_dataset,\n",
    "                                                                      epsilon, delta, search_var,\n",
    "                                                                      sample_num, eta_max, eta_min,\n",
    "                                                                      bound, k, query_limit)\n",
    "        query_counts_PIA.append(query_count_batch)\n",
    "        adv_images_PIA.append(adv_image_batch)\n",
    "        # count the number of success attack\n",
    "        success_count_PIA += sum(query_count <= query_limit for query_count in query_count_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a14b27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_image_all_pia = torch.concat(adv_images_PIA, dim = 0)\n",
    "query_count_all_pia = torch.concat(query_counts_PIA, dim = 0)\n",
    "torch.save(adv_image_all_pia, \"adv_img_PIA.pt\")\n",
    "torch.save(query_count_all_pia, \"query_PIA.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0a9181d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_count_all_pia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7875d6",
   "metadata": {},
   "source": [
    "# (7) Label-only Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo.attacker import PIA_adversarial_generator\n",
    "success_count_LO = 0\n",
    "query_counts_LO = []\n",
    "adv_images_LO = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[0]\n",
    "        adv_image_batch, query_count_batch = PIA_adversarial_generator(model, images, sample_img_dataset,\n",
    "                                                                      epsilon, delta, search_var,\n",
    "                                                                      sample_num, eta_max, eta_min,\n",
    "                                                                      bound, k, query_limit, label_only = True, mu = mu, m = m)\n",
    "        query_counts_LO.append(query_count_batch)\n",
    "        adv_images_LO.append(adv_image_batch)\n",
    "        # count the number of success attack\n",
    "        success_count_LO += sum(query_count <= query_limit for query_count in query_count_batch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count_LO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e32cb",
   "metadata": {},
   "source": [
    "# Playground + random testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b05e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_label = testset[3]\n",
    "test_img = test_img.unsqueeze(0)\n",
    "initial_img, initial_label = testset[111]\n",
    "initial_img = initial_img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c964d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adversarial: predicted class is {}'.format(reverse_mapping[torch.argmax(adv_logits, dim=1).detach().cpu().numpy()[0]]))\n",
    "print('Original: predicted class is {}'.format(reverse_mapping[torch.argmax(org_logits, dim=1).detach().cpu().numpy()[0]]))\n",
    "\n",
    "print('Adversarial: logit of true class is {:.2f}'.format(adv_logits[0, test_label]))\n",
    "print('Original: logit of true class is {:.2f}'.format(org_logits[0, test_label]))\n",
    "\n",
    "print('Adversarial: probability of true class is {:.2f}'.format(F.softmax(adv_logits, dim=1)[0, test_label]))\n",
    "print('Original: probability of true class is {:.2f}'.format(F.softmax(org_logits, dim=1)[0, test_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming x_adv is a PyTorch tensor with shape [C, H, W]\n",
    "# where C is the number of channels, H is the height, and W is the width.\n",
    "\n",
    "# Move the tensor to the CPU and convert to a NumPy array\n",
    "image_np1 = adv_img.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# If the image has more than one channel, transpose the dimensions from [C, H, W] to [H, W, C]\n",
    "if image_np1.ndim == 3:\n",
    "    # Transpose the image for plotting\n",
    "    image_np1 = image_np1.transpose(1, 2, 0)\n",
    "\n",
    "# If the image is in the range [0, 1], ensure it's scaled to [0, 255] if needed\n",
    "if image_np1.max() <= 1.0:\n",
    "    image_np1 = (image_np1 * 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef08391",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np = test_img.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# If the image has more than one channel, transpose the dimensions from [C, H, W] to [H, W, C]\n",
    "if image_np.ndim == 3:\n",
    "    # Transpose the image for plotting\n",
    "    image_np = image_np.transpose(1, 2, 0)\n",
    "\n",
    "# If the image is in the range [0, 1], ensure it's scaled to [0, 255] if needed\n",
    "if image_np.max() <= 1.0:\n",
    "    image_np = (image_np * 255).astype(np.uint8)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_np)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np2 = initial_img.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# If the image has more than one channel, transpose the dimensions from [C, H, W] to [H, W, C]\n",
    "if image_np2.ndim == 3:\n",
    "    # Transpose the image for plotting\n",
    "    image_np2 = image_np2.transpose(1, 2, 0)\n",
    "\n",
    "# If the image is in the range [0, 1], ensure it's scaled to [0, 255] if needed\n",
    "if image_np2.max() <= 1.0:\n",
    "    image_np2 = (image_np2 * 255).astype(np.uint8)\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_np1)\n",
    "plt.title('Adversarial Image_processing')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_np2)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2698cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, normal_mapping, reverse_mapping, sample_img_dataset = create_imagenet_dataset(img_reshape=(3, 224, 224), num_classes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf9325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.base import train_classifier\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Params\n",
    "batch_size=100\n",
    "lr = 0.0001\n",
    "device='cuda'\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf08259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3, use_auxiliary=True, num_classes=11):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.use_auxiliary = use_auxiliary\n",
    "        if use_auxiliary:\n",
    "            self.auxiliary4a = Auxiliary(512, num_classes)\n",
    "            self.auxiliary4d = Auxiliary(528, num_classes)\n",
    "        \n",
    "        self.inception3a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = None\n",
    "        z = None\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        if self.training and self.use_auxiliary:\n",
    "            y = self.auxiliary4a(x)\n",
    "        \n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        if self.training and self.use_auxiliary:\n",
    "            z = self.auxiliary4d(x)\n",
    "        \n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x, y, z\n",
    "class ConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, im_channels, num_1x1, num_3x3_red, num_3x3, num_5x5_red, num_5x5, num_pool_proj):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        self.one_by_one = ConvBlock(im_channels, num_1x1, kernel_size=1)\n",
    "        \n",
    "        self.tree_by_three_red = ConvBlock(im_channels, num_3x3_red, kernel_size=1)  \n",
    "        self.tree_by_three = ConvBlock(num_3x3_red, num_3x3, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.five_by_five_red = ConvBlock(im_channels, num_5x5_red, kernel_size=1)\n",
    "        self.five_by_five = ConvBlock(num_5x5_red, num_5x5, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_proj = ConvBlock(im_channels, num_pool_proj, kernel_size=1)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x1 = self.one_by_one(x)\n",
    "        \n",
    "        x2 = self.tree_by_three_red(x)\n",
    "        x2 = self.tree_by_three(x2)\n",
    "        \n",
    "        x3 = self.five_by_five_red(x)\n",
    "        x3 = self.five_by_five(x3)\n",
    "        \n",
    "        x4 = self.maxpool(x)\n",
    "        x4 = self.pool_proj(x4)\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3, x4], 1)\n",
    "        return x\n",
    "class Auxiliary(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Auxiliary, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv1x1 = ConvBlock(in_channels, 128, kernel_size=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fabd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Inception(num_classes=20)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f71cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4760dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=50, use_auxiliary=True):\n",
    "    import time\n",
    "    import copy\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train']: #, 'val']: # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]: # Iterate over data\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad() # Zero the parameter gradients\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'): # Forward. Track history if only in train\n",
    "                    \n",
    "                    if phase == 'train': # Backward + optimize only if in training phase\n",
    "                        if use_auxiliary:\n",
    "                            outputs, aux1, aux2 = model(inputs)\n",
    "                            loss = criterion(outputs, labels) + 0.3 * criterion(aux1, labels) + 0.3 * criterion(aux2, labels)\n",
    "                        else:\n",
    "                            outputs, _, _ = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            \n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    if phase == 'val':\n",
    "                        outputs, _, _ = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'val': # Adjust learning rate based on val loss\n",
    "                lr_scheduler.step(epoch_loss)\n",
    "                \n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc1d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 4.0827 Acc: 0.2560\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 3.3448 Acc: 0.3886\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 2.8830 Acc: 0.4746\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 2.4934 Acc: 0.5523\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.1464 Acc: 0.6304\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.8468 Acc: 0.6949\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.6660 Acc: 0.7294\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.5224 Acc: 0.7574\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.3635 Acc: 0.7918\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.1978 Acc: 0.8280\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.0177 Acc: 0.8688\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.8641 Acc: 0.8956\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.7361 Acc: 0.9241\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.6514 Acc: 0.9355\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.5540 Acc: 0.9514\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.4534 Acc: 0.9696\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.3682 Acc: 0.9810\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.3011 Acc: 0.9894\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.2449 Acc: 0.9936\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.1930 Acc: 0.9974\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.1586 Acc: 0.9974\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.1274 Acc: 0.9990\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0982 Acc: 0.9999\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0739 Acc: 0.9999\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0612 Acc: 0.9998\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0506 Acc: 0.9998\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0426 Acc: 0.9995\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0496 Acc: 0.9980\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0765 Acc: 0.9954\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.2924 Acc: 0.9583\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.4794 Acc: 0.9168\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3007 Acc: 0.9519\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.1366 Acc: 0.9868\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0709 Acc: 0.9959\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0611 Acc: 0.9958\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0488 Acc: 0.9965\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0288 Acc: 0.9989\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0201 Acc: 0.9995\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0138 Acc: 0.9999\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0104 Acc: 1.0000\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 1.0000\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 1.0000\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 1.0000\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 1.0000\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 1.0000\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0040 Acc: 1.0000\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0037 Acc: 1.0000\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 1.0000\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 1.0000\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 1.0000\n",
      "\n",
      "Training complete in 17m 16s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "model, _ = train_model(model, {\"train\": train_loader}, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b120bfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2959, -0.2882,  0.0228,  0.2408,  0.0149, -0.4518,  0.3257, -0.3219,\n",
       "           0.2708,  0.1396, -0.5786]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 0.7074, -0.0497, -0.1725, -0.4902, -0.0135, -0.2188, -0.1085, -0.1735,\n",
       "           0.0167,  0.1769, -0.0511]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2838, -0.4180, -0.3491,  0.2687, -0.0706,  0.0766, -0.2629,  0.1114,\n",
       "           0.2106, -0.4020,  0.0440]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(trainset[1][0].unsqueeze(0).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4202bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'imagenetclassifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515d9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
