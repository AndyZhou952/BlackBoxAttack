{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9798fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.base import eval_accuracy\n",
    "from utils.base import get_correct_predictions_subset\n",
    "from utils.data import create_butterfly_dataset\n",
    "from model.butterfly_classifier import DenseNet121\n",
    "from algo.attacker import adversarial_generator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eddbd495-a500-4a4d-b22b-a3b38ab89430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n"
     ]
    }
   ],
   "source": [
    "if torch.tensor([23]) != 1:\n",
    "    print('aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad73823-cf17-41e7-8ea9-ba9362f60f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.argmax(matches, dim=1)\n",
    "indices = indices + 1\n",
    "indices[~matches.any(dim=1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecb35e-261d-4a2a-a45e-d18943b3314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[~matches.any(dim=1)] = 0\n",
    "indices.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d5df9-07dd-44b7-b10e-b0b9a18f0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (idx == a).to(int)\n",
    "# indices: (batch_size, 1)\n",
    "indices = torch.argmax(matches, dim=1)\n",
    "indices = indices + 1\n",
    "indices[~matches.any(dim=1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954bc211-6bd0-43c9-a7b5-70ffce93ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "(idx == 0).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c2871-723c-454d-bb6f-66dd666afd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "(idx[0] == a).nonzero(as_tuple = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10eaf1-721d-46b4-a725-d12da708e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = idx[:, 0][4] \n",
    "def get_rank(x, indices):\n",
    "    vals = x[range(len(x)), indices]\n",
    "    return (x > vals[:, None]).long().sum(1)\n",
    "get_rank(idx, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e38d6",
   "metadata": {},
   "source": [
    "# (0) Download Butterfly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d gpiosenka/butterfly-images40-species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('butterfly-images40-species.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/butterfly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c431d",
   "metadata": {},
   "source": [
    "# (1) Process Butterfly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f5aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, normal_mapping, reverse_mapping, sample_img_dataset = create_butterfly_dataset()\n",
    "assert len(trainset) == 12594, 'Size of train set not match'\n",
    "assert len(testset) == 500, 'Size of test set not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64f944",
   "metadata": {},
   "source": [
    "# (2) Import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210a35cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DenseNet121(num_classes=len(normal_mapping)).to('cuda')\n",
    "model.load_state_dict(torch.load('./model/states/butterfly_classifier.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f6af3",
   "metadata": {},
   "source": [
    "# (3) Evaluate Untargeted Adversarial Examples\n",
    "\n",
    "We also subset the parts where the model could provide correct predictions to attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218a3cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.9640\n",
      "Number of correctly predicted samples: 482\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy, correct_subset = get_correct_predictions_subset(model, testset, batch_size=100)\n",
    "print('Accuracy on test set is {:.4f}'.format(accuracy))\n",
    "print('Number of correctly predicted samples:', len(correct_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cb3be",
   "metadata": {},
   "source": [
    "# (4) Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45bc647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters definition\n",
    "batch_size = 32\n",
    "query_limit = 30000 # max attack limit \n",
    "search_var = 1e-3 # amount to perturb the input image\n",
    "sample_num = 50 # 2*sample_num for estimating gradient\n",
    "bound = 0.1 # the l-infinity distance between the adversarial example and the input image\n",
    "# partial information paramters + lebel-only parameters\n",
    "epsilon = 0.5 # initial searching range from the target image\n",
    "delta = 0.01 # rate to decrease epsilon\n",
    "eta_max = 0.02 # maximum learning rate\n",
    "eta_min = 0.01 # minimum learning rate\n",
    "k = 5 # information access\n",
    "# label-only parameter\n",
    "mu = 0.001 # radius for sampling ball\n",
    "m = 50 # 2*number of sample for proxy score\n",
    "# test_loader = DataLoader(correct_subset, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(correct_subset, batch_size = 32, shuffle = False)\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce95b16",
   "metadata": {},
   "source": [
    "# (5) Query-Limited Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count = 0\n",
    "query_counts = []\n",
    "adv_images = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[0]\n",
    "        target_classes = batch[1]\n",
    "        adv_image_batch, query_count_batch = adversarial_generator(model, target_classes, images, \n",
    "                                                                 search_var, sample_num,\n",
    "                                                                bound, lr, query_limit)\n",
    "        query_counts.append(query_count_batch)\n",
    "        adv_images.append(adv_image_batch)\n",
    "        # count the number of success attack\n",
    "        success_count += sum(query_count <= query_limit for query_count in query_count_batch)\n",
    "        print(success_count)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79524075",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_image_all = torch.concat(adv_images, dim = 0)\n",
    "query_count_all = torch.concat(query_counts, dim = 0)\n",
    "torch.save(adv_image_all, \"adv_img.pt\")\n",
    "torch.save(query_count_all, \"query.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79e133",
   "metadata": {},
   "source": [
    "# (6) Partial-Information Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07f31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [42:46<00:00,  5.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from algo.attacker import PIA_adversarial_generator\n",
    "from algo.defender import PartialInfo\n",
    "from tqdm import trange\n",
    "\n",
    "success_count_PIA = 0\n",
    "query_counts_PIA = []\n",
    "adv_images_PIA = []\n",
    "\n",
    "top_k_output_model = PartialInfo(model=model, k=k)\n",
    "with torch.no_grad():\n",
    "    for i in trange(len(testset)):\n",
    "        \n",
    "        images = testset[i][0].unsqueeze(0)\n",
    "        # images = batch[0]\n",
    "        adv_image_batch, query_count_batch = PIA_adversarial_generator(top_k_output_model, images, sample_img_dataset,\n",
    "                                                                      epsilon, delta, search_var,\n",
    "                                                                      sample_num, eta_max, eta_min,\n",
    "                                                                      bound, k, query_limit)\n",
    "        query_counts_PIA.append(query_count_batch)\n",
    "        adv_images_PIA.append(adv_image_batch)\n",
    "        # count the number of success attack\n",
    "        success_count_PIA += sum(query_count <= query_limit for query_count in query_count_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2190580-3acf-4484-b222-0e0ef29adac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_image_all_pia = torch.concat(adv_images_PIA, dim = 0)\n",
    "query_count_all_pia = torch.concat(query_counts_PIA, dim = 0)\n",
    "torch.save(adv_image_all_pia, \"adv_img_PIA.pt\")\n",
    "torch.save(query_count_all_pia, \"query_PIA.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec916d8-74cb-403c-9e7a-d20f96ea0119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n",
       "        4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_count_all_pia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955b484",
   "metadata": {},
   "source": [
    "# (7) Label-only Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9772e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo.attacker import PIA_adversarial_generator\n",
    "success_count_LO = 0\n",
    "query_counts_LO = []\n",
    "adv_images_LO = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[0]\n",
    "        adv_image_batch, query_count_batch = PIA_adversarial_generator(model, images, sample_img_dataset,\n",
    "                                                                      epsilon, delta, search_var,\n",
    "                                                                      sample_num, eta_max, eta_min,\n",
    "                                                                      bound, k, query_limit, label_only = True, mu = mu, m = m)\n",
    "        query_counts_LO.append(query_count_batch)\n",
    "        adv_images_LO.append(adv_image_batch)\n",
    "        # count the number of success attack\n",
    "        success_count_LO += sum(query_count <= query_limit for query_count in query_count_batch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8aff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count_LO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a4c1e",
   "metadata": {},
   "source": [
    "# Playground + random testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_label = testset[3]\n",
    "test_img = test_img.unsqueeze(0)\n",
    "initial_img, initial_label = testset[111]\n",
    "initial_img = initial_img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc667585",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adversarial: predicted class is {}'.format(reverse_mapping[torch.argmax(adv_logits, dim=1).detach().cpu().numpy()[0]]))\n",
    "print('Original: predicted class is {}'.format(reverse_mapping[torch.argmax(org_logits, dim=1).detach().cpu().numpy()[0]]))\n",
    "\n",
    "print('Adversarial: logit of true class is {:.2f}'.format(adv_logits[0, test_label]))\n",
    "print('Original: logit of true class is {:.2f}'.format(org_logits[0, test_label]))\n",
    "\n",
    "print('Adversarial: probability of true class is {:.2f}'.format(F.softmax(adv_logits, dim=1)[0, test_label]))\n",
    "print('Original: probability of true class is {:.2f}'.format(F.softmax(org_logits, dim=1)[0, test_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming x_adv is a PyTorch tensor with shape [C, H, W]\n",
    "# where C is the number of channels, H is the height, and W is the width.\n",
    "\n",
    "# Move the tensor to the CPU and convert to a NumPy array\n",
    "image_np1 = adv_img.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# If the image has more than one channel, transpose the dimensions from [C, H, W] to [H, W, C]\n",
    "if image_np1.ndim == 3:\n",
    "    # Transpose the image for plotting\n",
    "    image_np1 = image_np1.transpose(1, 2, 0)\n",
    "\n",
    "# If the image is in the range [0, 1], ensure it's scaled to [0, 255] if needed\n",
    "if image_np1.max() <= 1.0:\n",
    "    image_np1 = (image_np1 * 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a316e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np = test_img.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# If the image has more than one channel, transpose the dimensions from [C, H, W] to [H, W, C]\n",
    "if image_np.ndim == 3:\n",
    "    # Transpose the image for plotting\n",
    "    image_np = image_np.transpose(1, 2, 0)\n",
    "\n",
    "# If the image is in the range [0, 1], ensure it's scaled to [0, 255] if needed\n",
    "if image_np.max() <= 1.0:\n",
    "    image_np = (image_np * 255).astype(np.uint8)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_np)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np2 = initial_img.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# If the image has more than one channel, transpose the dimensions from [C, H, W] to [H, W, C]\n",
    "if image_np2.ndim == 3:\n",
    "    # Transpose the image for plotting\n",
    "    image_np2 = image_np2.transpose(1, 2, 0)\n",
    "\n",
    "# If the image is in the range [0, 1], ensure it's scaled to [0, 255] if needed\n",
    "if image_np2.max() <= 1.0:\n",
    "    image_np2 = (image_np2 * 255).astype(np.uint8)\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_np1)\n",
    "plt.title('Adversarial Image_processing')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_np2)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
