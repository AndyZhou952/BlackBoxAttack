{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45ae806",
   "metadata": {},
   "source": [
    "# (0) Download Butterfly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002f6b3",
   "metadata": {},
   "source": [
    "# (1) Process Butterfly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "939b08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import timm\n",
    "device = 'cuda' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f7bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, path_label, transform=None):\n",
    "        self.path_label = path_label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.path_label[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "# Create dataset from raw structure obtained from Kaggle\n",
    "# (1): Download dataset fron Kaggle: https://www.kaggle.com/datasets/gpiosenka/butterfly-images40-species?select=train\n",
    "# (2): Unzip the downloaded dataset to './data/butterfly'\n",
    "# (3): Run the function\n",
    "def create_dataset(path = './data/butterfly/'):\n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.Resize((224,224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "    train_path = path + 'train'\n",
    "    test_path = path + 'test'\n",
    "    \n",
    "    class_names=sorted(os.listdir(train_path))\n",
    "    N=list(range(len(class_names)))\n",
    "    normal_mapping=dict(zip(class_names,N)) \n",
    "    reverse_mapping=dict(zip(N,class_names))\n",
    "\n",
    "    paths0=[]\n",
    "    for dirname, _, filenames in os.walk(train_path):\n",
    "        for filename in filenames:\n",
    "            if filename[-4:]=='.jpg':\n",
    "                path=os.path.join(dirname, filename)\n",
    "                label=dirname.split('\\\\')[-1]\n",
    "                if label == '.ipynb_checkpoints':\n",
    "                    continue\n",
    "                paths0+=[(path,normal_mapping[label])]\n",
    "            \n",
    "    tpaths0=[]\n",
    "    for dirname, _, filenames in os.walk(test_path):\n",
    "        for filename in filenames:\n",
    "            if filename[-4:]=='.jpg':\n",
    "                path=os.path.join(dirname, filename)\n",
    "                label=dirname.split('\\\\')[-1]\n",
    "                if label == '.ipynb_checkpoints':\n",
    "                    continue\n",
    "                tpaths0+=[(path,normal_mapping[label])]\n",
    "\n",
    "    random.shuffle(paths0)            \n",
    "    random.shuffle(tpaths0)  \n",
    "\n",
    "    \n",
    "    trainset = ImageDataset(paths0, transform)\n",
    "    testset = ImageDataset(tpaths0, transform)\n",
    "    \n",
    "    return trainset, testset, normal_mapping, reverse_mapping\n",
    "\n",
    "\n",
    "trainset, testset, normal_mapping, reverse_mapping = create_dataset()\n",
    "assert len(trainset) == 12594, 'Size of train set not match'\n",
    "assert len(testset) == 500, 'Size of test set not match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba358e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, model='densenet121', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model, pretrained, in_chans=3)\n",
    "        self.fc1 = nn.Linear(1000,32)\n",
    "        self.fc2 = nn.Linear(32,64)        \n",
    "        self.fc3 = nn.Linear(64,num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "model = DenseNet121(num_classes=len(normal_mapping))\n",
    "model.load_state_dict(torch.load('butterfly_classifier.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e3dea",
   "metadata": {},
   "source": [
    "# NES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff289d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query limited attack using NES\n",
    "def NES(model, target_class, image, search_var, sample_num, device, g, u_i):\n",
    "    #parameters\n",
    "    n = sample_num #should be even\n",
    "    N = image.size(2) #assume the image is N x N may subject to change\n",
    "\n",
    "    #NES estimation\n",
    "    g.zero_()\n",
    "    for i in range(n):\n",
    "        u_i.normal_()\n",
    "        g = g + F.softmax(model(image + search_var * u_i), dim =1)[0,target_class] * u_i\n",
    "        g = g - F.softmax(model(image - search_var * u_i), dim =1)[0,target_class] * u_i #we assume the output of the model is ordered by class index\n",
    "    return 1 / (2*n*search_var) * g\n",
    "\n",
    "def adversarial_generator(model, target_class, image, search_var, sample_num, bound, lr, query_limit, device):\n",
    "    from tqdm import tqdm \n",
    "    image = image.to(device)\n",
    "    adv_image = image.clone()\n",
    "    adv_image = adv_image.to(device)\n",
    "    \n",
    "    N = image.size(2)\n",
    "    g = torch.zeros(N).to(device)\n",
    "    u_i = torch.randn((N,N)).to(device)\n",
    "    for i in tqdm(range(query_limit // sample_num)):\n",
    "        gradient = NES(model, target_class, adv_image, search_var, sample_num, device, g, u_i)\n",
    "        tmp = adv_image - lr * torch.sign(gradient)\n",
    "        #projected gradient descent\n",
    "        adv_image = torch.max(torch.min(tmp, image + bound), image - bound)\n",
    "    return adv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e8c656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9480, device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(dataset=testset, batch_size=100)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inp, label) in enumerate(test_loader):\n",
    "        inp, label = inp.to(device), label.to(device)\n",
    "        output = model(inp)\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        count += (output == label).sum()\n",
    "count / 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd04183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "a = NES(model, 20, testset[0][0].unsqueeze(0), 1, 10, 'cuda')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3d274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▍                                                                      | 3/20 [00:20<01:55,  6.80s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.99 GiB of which 0 bytes is free. Of the allocated memory 37.94 GiB is allocated by PyTorch, and 307.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fake \u001b[38;5;241m=\u001b[39m \u001b[43madversarial_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36madversarial_generator\u001b[1;34m(model, target_class, image, search_var, sample_num, bound, lr, query_limit, device)\u001b[0m\n\u001b[0;32m     20\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m adv_image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(query_limit \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m sample_num)):\n\u001b[1;32m---> 22\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[43mNES\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m adv_image \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(gradient)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#projected gradient descent\u001b[39;00m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mNES\u001b[1;34m(model, target_class, image, search_var, sample_num, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m     11\u001b[0m     u_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((N,N))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m     g \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msearch_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu_i\u001b[49m\u001b[43m)\u001b[49m, dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m,target_class] \u001b[38;5;241m*\u001b[39m u_i\n\u001b[0;32m     13\u001b[0m     g \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(model(image \u001b[38;5;241m-\u001b[39m search_var \u001b[38;5;241m*\u001b[39m u_i), dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m,target_class] \u001b[38;5;241m*\u001b[39m u_i \u001b[38;5;66;03m#we assume the output of the model is ordered by class index\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mn\u001b[38;5;241m*\u001b[39msearch_var) \u001b[38;5;241m*\u001b[39m g\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mDenseNet121.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\timm\\models\\densenet.py:302\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 302\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_pool(x)\n\u001b[0;32m    304\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_drop(x)\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\timm\\models\\densenet.py:299\u001b[0m, in \u001b[0;36mDenseNet.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\timm\\models\\densenet.py:124\u001b[0m, in \u001b[0;36mDenseBlock.forward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    122\u001b[0m features \u001b[38;5;241m=\u001b[39m [init_features]\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 124\u001b[0m     new_features \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(new_features)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\timm\\models\\densenet.py:88\u001b[0m, in \u001b[0;36mDenseLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m new_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(bottleneck_output))\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mF:\\anaconda\\lib\\site-packages\\timm\\models\\densenet.py:45\u001b[0m, in \u001b[0;36mDenseLayer.bottleneck_fn\u001b[1;34m(self, xs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbottleneck_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# type: (List[torch.Tensor]) -> torch.Tensor\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     concated_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(concated_features))  \u001b[38;5;66;03m# noqa: T484\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bottleneck_output\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.99 GiB of which 0 bytes is free. Of the allocated memory 37.94 GiB is allocated by PyTorch, and 307.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "fake = adversarial_generator(model, 20, testset[0][0].unsqueeze(0), 1e-3, 50, 0.05, 0.01, 1000, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3970032",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(model(fake))\n",
    "torch.argmax(model(testset[0][0].unsqueeze(0).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(model(testset[0][0].unsqueeze(0).to(device)))\n",
    "model.to('cpu')\n",
    "adversarial_generator(model, 20, testset[0][0].unsqueeze(0), 1, 50, 1, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(testset[0][0].unsqueeze(0))[0,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc24ba",
   "metadata": {},
   "source": [
    "# Partial Information Attacker (PIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44927935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_top_classes(model, image, topk=1):\n",
    "    \"\"\"\n",
    "    Function to get the top predicted classes from the model.\n",
    "    Assumes the model outputs logits and you can get the topk classes.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        topk_probs, topk_classes = torch.topk(logits, topk)\n",
    "    return topk_classes.cpu().numpy()\n",
    "\n",
    "def partial_information_attack(model, original_image, target_image, target_label, alpha=0.01, beta=0.005, iterations=100, topk=1):\n",
    "    \"\"\"\n",
    "    Perform the Partial Information Attack.\n",
    "    \"\"\"\n",
    "    adversarial_image = target_image.clone()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        blended_image = (1 - alpha) * adversarial_image + alpha * original_image\n",
    "\n",
    "        top_classes = predict_top_classes(model, blended_image, topk=topk)\n",
    "\n",
    "        # Check if target label is in top classes\n",
    "        if target_label in top_classes:\n",
    "            # Increase original image content\n",
    "            alpha = min(alpha + beta, 1)\n",
    "        else:\n",
    "            # Increase target image content\n",
    "            alpha = max(alpha - beta, 0)\n",
    "\n",
    "        adversarial_image = blended_image\n",
    "\n",
    "    return adversarial_image\n",
    "\n",
    "# Load the original and target images (same as before)\n",
    "original_image, target_label = testset[0]  # Replace 0 with the index of your choice\n",
    "target_class = reverse_mapping[target_label]  # Replace with your target class name\n",
    "target_image, _ = next((img, lbl) for img, lbl in trainset if lbl == target_label)\n",
    "\n",
    "# Add batch dimension and move to the same device as the model\n",
    "original_image_batch = original_image.unsqueeze(0).to(device)\n",
    "target_image_batch = target_image.unsqueeze(0).to(device)\n",
    "\n",
    "# Run the Partial Information Attack\n",
    "adversarial_image = partial_information_attack(model, original_image_batch, target_image_batch, target_label, alpha=0.01, beta=0.005, iterations=100, topk=1)\n",
    "\n",
    "# Convert images back to PIL for visualization\n",
    "transform_back = transforms.ToPILImage()\n",
    "\n",
    "original_image_pil = transform_back(original_image)\n",
    "adversarial_image_pil = transform_back(adversarial_image.squeeze(0).cpu())\n",
    "\n",
    "# Display original and adversarial images\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image_pil)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(adversarial_image_pil)\n",
    "plt.title('Adversarial Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor that has already been denormalized.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))  # Convert from PyTorch to numpy format and rearrange color channels\n",
    "    inp = np.clip(inp, 0, 1)  # Ensure values are within [0, 1] for proper display\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # Pause a bit so that plots are updated\n",
    "\n",
    "# Assuming original_image and adversarial_image are your images\n",
    "\n",
    "# Reverse normalization for visualization\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1).to(tensor.device)\n",
    "    tensor = tensor * std + mean  # reverse the normalization\n",
    "    tensor = tensor.clamp(0, 1)   # clamp the values to be between 0 and 1\n",
    "    return tensor\n",
    "\n",
    "# Denormalize images\n",
    "original_image_denorm = denormalize(original_image.squeeze(0))\n",
    "adversarial_image_denorm = denormalize(adversarial_image.squeeze(0))\n",
    "\n",
    "# Move tensors to the same device (e.g., CPU for imshow)\n",
    "original_image_denorm = original_image_denorm.cpu()\n",
    "adversarial_image_denorm = adversarial_image_denorm.cpu()\n",
    "\n",
    "# Create a grid of images\n",
    "image_grid = torchvision.utils.make_grid([original_image_denorm, adversarial_image_denorm])\n",
    "\n",
    "# Show images\n",
    "plt.figure(figsize=(10, 5))\n",
    "imshow(image_grid, title=['Original Image', 'Adversarial Image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5adde7",
   "metadata": {},
   "source": [
    "## Query Limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc88a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query limited attack using NES\n",
    "def NES(model, target_class, image, search_var, sample_num):\n",
    "    #parameters\n",
    "    n = sample_num #should be even\n",
    "    N = image.size(0) #assume the image is n x N x N may subject to change\n",
    "\n",
    "    #NES estimation\n",
    "    g = torch.zeros(n)\n",
    "    for i in range(n):\n",
    "        u_i = torch.randn((N,N))\n",
    "        g = g + model(image + search_var * u_i)[target_class] * u_i\n",
    "        g = g - model(image - search_var * u_i)[target_class] * u_i #we assume the output of the model is ordered by class index\n",
    "    return 1 / (2*n*search_var) * g\n",
    "\n",
    "def adversarial_generator(model, target_class, image, search_var, sample_num, bound, lr, query_limit):\n",
    "    adv_image = image.copy()\n",
    "    for i in range(query_limit // sample_num):\n",
    "        gradient = NES(model, target_class, adv_image, search_var, sample_num)\n",
    "        tmp = adv_image - lr * torch.sign(gradient)\n",
    "        #projected gradient descent\n",
    "        adv_image = torch.max(torch.min(adv_image, image + bound), image - bound)\n",
    "    return adv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fe795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
