{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e22bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.base import eval_accuracy\n",
    "from utils.base import get_correct_predictions_subset\n",
    "from utils.data import create_butterfly_dataset\n",
    "from utils.data import create_imagenet_dataset\n",
    "from model.imagenet_classifier import InceptionV3\n",
    "from algo.attacker import adversarial_generator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from utils.base import quick_predict\n",
    "from algo.attacker import PIA_adversarial_generator\n",
    "from algo.defender import PartialInfo\n",
    "from tqdm import trange\n",
    "from algo.defender import AAAProtectedClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193944a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (0) Download Tiny ImageNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f98337",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d akash2sharma/tiny-imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('tiny-imagenet.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/tinyimagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd55f2",
   "metadata": {},
   "source": [
    "# (1) Process ImageNet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4b5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, normal_mapping, reverse_mapping, sample_img_dataset = create_imagenet_dataset(img_reshape=(3, 224, 224), num_classes = 20)\n",
    "assert len(trainset) == 8000, 'Size of train set not match'\n",
    "assert len(testset) == 2000, 'Size of test set not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d8426",
   "metadata": {},
   "source": [
    "# (2) Import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1da75a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InceptionV3(num_classes=20).to('cuda')\n",
    "model.load_state_dict(torch.load('./model/states/imagenetclassifier.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a5ec8",
   "metadata": {},
   "source": [
    "# (3) Evaluate Untargeted Adversarial Examples\n",
    "\n",
    "We also subset the parts where the model could provide correct predictions to attack. We subset 500 images for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20445d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.9730\n",
      "Number of correctly predicted samples: 1946\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy, correct_subset = get_correct_predictions_subset(model, testset, batch_size=100)\n",
    "print('Accuracy on test set is {:.4f}'.format(accuracy))\n",
    "print('Number of correctly predicted samples:', len(correct_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2c4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset only 500 for processing\n",
    "subset_indices = correct_subset.indices\n",
    "random_indices = random.sample(subset_indices, 500)\n",
    "correct_subset = torch.utils.data.Subset(correct_subset.dataset, random_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a3835",
   "metadata": {},
   "source": [
    "# (4) Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3a1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attacker params\n",
    "batch_size = 32\n",
    "query_limit = 30000 # max attack limit \n",
    "LO_query_limit = 500000 # max attack limit \n",
    "search_var = 1e-3 # amount to perturb the input image\n",
    "sample_num = 50 # 2*sample_num for estimating gradient\n",
    "bound = 0.1 # the l-infinity distance between the adversarial example and the input image\n",
    "# partial information paramters + lebel-only parameters\n",
    "epsilon = 0.5 # initial searching range from the target image\n",
    "delta = 0.01 # rate to decrease epsilon\n",
    "eta_max = 0.02 # maximum learning rate\n",
    "eta_min = 0.01 # minimum learning rate\n",
    "k = 5 # information access\n",
    "# label-only parameter\n",
    "mu = 0.001 # radius for sampling ball\n",
    "m = 50 # 2*number of sample for proxy score\n",
    "#correct_subset_loader = DataLoader(correct_subset, batch_size = batch_size, shuffle = False)\n",
    "correct_subset_loader = DataLoader(correct_subset, batch_size = 5, shuffle = False)\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b071f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinga\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# defender params\n",
    "alpha=1\n",
    "tau=80\n",
    "kappa=100\n",
    "T=1\n",
    "beta=5\n",
    "lr=0.1\n",
    "\n",
    "model.eval()\n",
    "protected_model = AAAProtectedClassifier(model=model, alpha=alpha, tau=tau, kappa=kappa, T=T, beta=beta, lr=lr, AAA_type='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbba7cc",
   "metadata": {},
   "source": [
    "# (5) Query-Limited Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95cac004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [35:40<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success count is 485.0, total count is 500, success attack rate is 0.97\n"
     ]
    }
   ],
   "source": [
    "from utils.base import quick_predict\n",
    "\n",
    "protected_model.to('cuda')\n",
    "protected_model.eval()\n",
    "\n",
    "success_count = 0\n",
    "query_counts = []\n",
    "adv_images = []\n",
    "with torch.no_grad():\n",
    "    for i in trange(len(correct_subset)):\n",
    "        images = correct_subset[i][0].unsqueeze(0)\n",
    "        \n",
    "        target_classes = torch.tensor(correct_subset[i][1]).view(1).to('cuda')\n",
    "        adv_image_batch, query_count_batch = adversarial_generator(protected_model, target_classes, images, \n",
    "                                                                 search_var, sample_num,\n",
    "                                                                bound, lr, query_limit)\n",
    "        query_counts.append(query_count_batch)\n",
    "        adv_images.append(adv_image_batch)\n",
    "        \n",
    "        adv_class = quick_predict(protected_model, adv_image_batch)\n",
    "        success_count += (adv_class != target_classes).to(float).sum()\n",
    "adv_image_all = torch.concat(adv_images, dim = 0)\n",
    "query_count_all = torch.concat(query_counts, dim = 0)\n",
    "torch.save(adv_image_all, \"QL_adv_img_defense_net.pt\")\n",
    "torch.save(query_count_all, \"QL_query_defense_net.pt\")\n",
    "print('Success count is {}, total count is {}, success attack rate is {}'.format(success_count, len(correct_subset), success_count /len(correct_subset) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b974b",
   "metadata": {},
   "source": [
    "# (6) Partial-Information Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe29e954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████▍                                                             | 11/50 [36:57<2:11:01, 201.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m images \u001b[38;5;241m=\u001b[39m correct_subset[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# images = batch[0]\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m adv_image_batch, query_count_batch \u001b[38;5;241m=\u001b[39m \u001b[43mPIA_adversarial_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotected_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_img_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43msample_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m query_counts_PIA\u001b[38;5;241m.\u001b[39mappend(query_count_batch)\n\u001b[0;32m     14\u001b[0m adv_images_PIA\u001b[38;5;241m.\u001b[39mappend(adv_image_batch)\n",
      "File \u001b[1;32m~\\Jupyter\\BlackBoxAttack\\algo\\attacker.py:321\u001b[0m, in \u001b[0;36mPIA_adversarial_generator\u001b[1;34m(model, images, sample_img_dataset, epsilon, delta, search_var, sample_num, eta_max, eta_min, bound, k, query_limit, label_only, mu, m)\u001b[0m\n\u001b[0;32m    318\u001b[0m x_adv_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(x_adv_hat, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# while not get_rank_of_target(model, x_adv_hat, y_adv, k):\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mget_rank_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_adv_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eta \u001b[38;5;241m<\u001b[39m eta_min:\n\u001b[0;32m    323\u001b[0m         epsilon \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m delta\n",
      "File \u001b[1;32m~\\Jupyter\\BlackBoxAttack\\algo\\attacker.py:170\u001b[0m, in \u001b[0;36mget_rank_of_target\u001b[1;34m(model, image, target_class, k)\u001b[0m\n\u001b[0;32m    167\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# output: (batch_size, K) <-- Big K: number of classes\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Jupyter\\BlackBoxAttack\\algo\\defender.py:66\u001b[0m, in \u001b[0;36mAAAProtectedClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     63\u001b[0m     org_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[0;32m     64\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m org_logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m protected_logits \u001b[38;5;241m=\u001b[39m \u001b[43mAAA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morg_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m                       \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mAAA_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAAA_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m protected_logits\n",
      "File \u001b[1;32m~\\Jupyter\\BlackBoxAttack\\algo\\defender.py:40\u001b[0m, in \u001b[0;36mAAA\u001b[1;34m(z, y, L, alpha, tau, kappa, T, beta, lr, AAA_type)\u001b[0m\n\u001b[0;32m     38\u001b[0m         loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(L(u, y) \u001b[38;5;241m-\u001b[39m l_trg) \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(torch\u001b[38;5;241m.\u001b[39mmax(F\u001b[38;5;241m.\u001b[39msoftmax(u, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m-\u001b[39m p_trg)\n\u001b[0;32m     39\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 40\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "success_count_PIA = 0\n",
    "query_counts_PIA = []\n",
    "adv_images_PIA = []\n",
    "with torch.no_grad():\n",
    "    for i in trange(len(correct_subset) - 450):\n",
    "        \n",
    "        images = correct_subset[i][0].unsqueeze(0)\n",
    "        # images = batch[0]\n",
    "        adv_image_batch, query_count_batch = PIA_adversarial_generator(protected_model, images, sample_img_dataset,\n",
    "                                                                      epsilon, delta, search_var,\n",
    "                                                                      sample_num, eta_max, eta_min,\n",
    "                                                                      bound, k, query_limit, label_only = False)\n",
    "        query_counts_PIA.append(query_count_batch)\n",
    "        adv_images_PIA.append(adv_image_batch)\n",
    "        \n",
    "        \n",
    "        reach_bound = (query_count_batch < query_limit).to('cuda')\n",
    "        successful_attack = (quick_predict(protected_model, adv_image_batch) != correct_subset[i][1]).to('cuda')\n",
    "        success_count_PIA += (successful_attack * reach_bound).to(int).sum()\n",
    "        \n",
    "        \n",
    "adv_image_all_pia = torch.concat(adv_images_PIA, dim = 0)\n",
    "query_count_all_pia = torch.concat(query_counts_PIA, dim = 0)\n",
    "torch.save(adv_image_all_pia, \"PIA_adv_img_defense_net.pt\")\n",
    "torch.save(query_count_all_pia, \"PIA_query.pt\")\n",
    "print('Success count is {}, total count is {}, success attack rate is {}'.format(success_count_PIA, len(correct_subset), success_count_PIA /len(correct_subset) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87affde7",
   "metadata": {},
   "source": [
    "# (7) Label-only Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87008d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count_LO = 0\n",
    "query_counts_LO = []\n",
    "adv_images_LO = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in trange(10):\n",
    "        images = correct_subset[i][0].unsqueeze(0)\n",
    "        adv_image_batch, query_count_batch = PIA_adversarial_generator(model, images, sample_img_dataset,\n",
    "                                                                      epsilon, delta, search_var,\n",
    "                                                                      sample_num, eta_max, eta_min,\n",
    "                                                                      bound, k, query_limit=LO_query_limit, label_only = True, mu = mu, m = m) # different query_limit\n",
    "        query_counts_LO.append(query_count_batch)\n",
    "        adv_images_LO.append(adv_image_batch)\n",
    "        \n",
    "        reach_bound = (query_count_batch < LO_query_limit).to('cuda')\n",
    "        successful_attack = (quick_predict(model, adv_image_batch) != correct_subset[i][1]).to('cuda')\n",
    "        success_count_LO += (successful_attack * reach_bound).to(int).sum()\n",
    "\n",
    "        \n",
    "adv_image_all_lo = torch.concat(adv_images_LO, dim = 0)\n",
    "query_count_all_lo = torch.concat(query_counts_LO, dim = 0)\n",
    "torch.save(adv_image_all_lo, \"LO_adv_img.pt\")\n",
    "torch.save(query_count_all_lo, \"LO_query.pt\")\n",
    "print('Success count is {}, total count is {}, success attack rate is {}'.format(success_count_PIA, 10, success_count /10 ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
